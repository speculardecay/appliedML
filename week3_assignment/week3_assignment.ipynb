{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f6ecb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e7f3428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download training data in the \"data\" folder\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root = r\"D:\\ML\\ML for 3D and VFX Course\\Week3\\dev\\data\",\n",
    "    train = True,\n",
    "    download = True,\n",
    "    transform = transforms.ToTensor()\n",
    ")\n",
    "\n",
    "# Download test data in the \"data\" folder\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root = r\"D:\\ML\\ML for 3D and VFX Course\\Week3\\dev\\data\",\n",
    "    train = False,\n",
    "    download = True,\n",
    "    transform = transforms.ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adae3642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9eac79d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv_stack): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=3136, out_features=128, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.4, inplace=False)\n",
      "    (4): Linear(in_features=128, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv_stack = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64*7*7, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(128, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_stack(x)\n",
    "        return self.classifier(x)\n",
    "\n",
    "model = CNN().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "275098c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "# Learning rate\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    # Switch model to train\n",
    "    model.train()\n",
    "\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        pred = model(X)\n",
    "        # pred = prediction, y = correct value\n",
    "        # loss = how far are we from correct value\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Gradient descent\n",
    "        loss.backward()\n",
    "        # Update the model based on gradient descent\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch*len(X)\n",
    "            print(f\"loss: {loss:>7f} [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    # Switch model to test / evaluate\n",
    "    model.eval()\n",
    "    test_loss, correct = 0,0\n",
    "    with torch.inference_mode():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test error : \\n Accuracy : {(100*correct):>0.1f}%, Avg loss : {test_loss:>8f} \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "143b2240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      " ---\n",
      "loss: 0.135467 [    0/60000]\n",
      "loss: 0.268675 [ 6400/60000]\n",
      "loss: 0.123672 [12800/60000]\n",
      "loss: 0.260538 [19200/60000]\n",
      "loss: 0.196017 [25600/60000]\n",
      "loss: 0.357391 [32000/60000]\n",
      "loss: 0.234206 [38400/60000]\n",
      "loss: 0.286093 [44800/60000]\n",
      "loss: 0.224237 [51200/60000]\n",
      "loss: 0.190776 [57600/60000]\n",
      "Test error : \n",
      " Accuracy : 93.6%, Avg loss : 0.169966 \n",
      "\n",
      "Epoch 2\n",
      " ---\n",
      "loss: 0.194487 [    0/60000]\n",
      "loss: 0.202613 [ 6400/60000]\n",
      "loss: 0.136965 [12800/60000]\n",
      "loss: 0.223566 [19200/60000]\n",
      "loss: 0.213388 [25600/60000]\n",
      "loss: 0.313961 [32000/60000]\n",
      "loss: 0.193165 [38400/60000]\n",
      "loss: 0.256858 [44800/60000]\n",
      "loss: 0.230133 [51200/60000]\n",
      "loss: 0.168074 [57600/60000]\n",
      "Test error : \n",
      " Accuracy : 93.9%, Avg loss : 0.164011 \n",
      "\n",
      "Epoch 3\n",
      " ---\n",
      "loss: 0.112735 [    0/60000]\n",
      "loss: 0.211023 [ 6400/60000]\n",
      "loss: 0.104233 [12800/60000]\n",
      "loss: 0.223221 [19200/60000]\n",
      "loss: 0.124442 [25600/60000]\n",
      "loss: 0.290542 [32000/60000]\n",
      "loss: 0.177105 [38400/60000]\n",
      "loss: 0.292884 [44800/60000]\n",
      "loss: 0.234192 [51200/60000]\n",
      "loss: 0.156762 [57600/60000]\n",
      "Test error : \n",
      " Accuracy : 94.0%, Avg loss : 0.159356 \n",
      "\n",
      "Epoch 4\n",
      " ---\n",
      "loss: 0.134737 [    0/60000]\n",
      "loss: 0.167763 [ 6400/60000]\n",
      "loss: 0.107942 [12800/60000]\n",
      "loss: 0.214476 [19200/60000]\n",
      "loss: 0.189445 [25600/60000]\n",
      "loss: 0.343933 [32000/60000]\n",
      "loss: 0.172121 [38400/60000]\n",
      "loss: 0.275971 [44800/60000]\n",
      "loss: 0.213592 [51200/60000]\n",
      "loss: 0.145778 [57600/60000]\n",
      "Test error : \n",
      " Accuracy : 94.2%, Avg loss : 0.155454 \n",
      "\n",
      "Epoch 5\n",
      " ---\n",
      "loss: 0.127949 [    0/60000]\n",
      "loss: 0.197889 [ 6400/60000]\n",
      "loss: 0.081614 [12800/60000]\n",
      "loss: 0.254184 [19200/60000]\n",
      "loss: 0.105389 [25600/60000]\n",
      "loss: 0.233926 [32000/60000]\n",
      "loss: 0.167663 [38400/60000]\n",
      "loss: 0.198868 [44800/60000]\n",
      "loss: 0.196901 [51200/60000]\n",
      "loss: 0.133990 [57600/60000]\n",
      "Test error : \n",
      " Accuracy : 94.4%, Avg loss : 0.151742 \n",
      "\n",
      "Epoch 6\n",
      " ---\n",
      "loss: 0.139418 [    0/60000]\n",
      "loss: 0.210105 [ 6400/60000]\n",
      "loss: 0.081054 [12800/60000]\n",
      "loss: 0.242724 [19200/60000]\n",
      "loss: 0.148793 [25600/60000]\n",
      "loss: 0.298016 [32000/60000]\n",
      "loss: 0.153869 [38400/60000]\n",
      "loss: 0.223762 [44800/60000]\n",
      "loss: 0.175314 [51200/60000]\n",
      "loss: 0.221414 [57600/60000]\n",
      "Test error : \n",
      " Accuracy : 94.5%, Avg loss : 0.148531 \n",
      "\n",
      "Epoch 7\n",
      " ---\n",
      "loss: 0.104881 [    0/60000]\n",
      "loss: 0.207323 [ 6400/60000]\n",
      "loss: 0.069088 [12800/60000]\n",
      "loss: 0.150089 [19200/60000]\n",
      "loss: 0.143878 [25600/60000]\n",
      "loss: 0.339401 [32000/60000]\n",
      "loss: 0.176646 [38400/60000]\n",
      "loss: 0.200675 [44800/60000]\n",
      "loss: 0.167623 [51200/60000]\n",
      "loss: 0.126191 [57600/60000]\n",
      "Test error : \n",
      " Accuracy : 94.6%, Avg loss : 0.145357 \n",
      "\n",
      "Epoch 8\n",
      " ---\n",
      "loss: 0.168868 [    0/60000]\n",
      "loss: 0.176816 [ 6400/60000]\n",
      "loss: 0.071630 [12800/60000]\n",
      "loss: 0.175310 [19200/60000]\n",
      "loss: 0.111192 [25600/60000]\n",
      "loss: 0.268598 [32000/60000]\n",
      "loss: 0.225370 [38400/60000]\n",
      "loss: 0.222834 [44800/60000]\n",
      "loss: 0.191753 [51200/60000]\n",
      "loss: 0.160651 [57600/60000]\n",
      "Test error : \n",
      " Accuracy : 94.8%, Avg loss : 0.141696 \n",
      "\n",
      "Epoch 9\n",
      " ---\n",
      "loss: 0.143688 [    0/60000]\n",
      "loss: 0.251774 [ 6400/60000]\n",
      "loss: 0.065185 [12800/60000]\n",
      "loss: 0.218884 [19200/60000]\n",
      "loss: 0.139345 [25600/60000]\n",
      "loss: 0.265128 [32000/60000]\n",
      "loss: 0.199350 [38400/60000]\n",
      "loss: 0.234973 [44800/60000]\n",
      "loss: 0.222638 [51200/60000]\n",
      "loss: 0.125293 [57600/60000]\n",
      "Test error : \n",
      " Accuracy : 94.8%, Avg loss : 0.140238 \n",
      "\n",
      "Epoch 10\n",
      " ---\n",
      "loss: 0.146587 [    0/60000]\n",
      "loss: 0.228044 [ 6400/60000]\n",
      "loss: 0.050415 [12800/60000]\n",
      "loss: 0.190470 [19200/60000]\n",
      "loss: 0.117224 [25600/60000]\n",
      "loss: 0.356350 [32000/60000]\n",
      "loss: 0.148719 [38400/60000]\n",
      "loss: 0.205170 [44800/60000]\n",
      "loss: 0.177795 [51200/60000]\n",
      "loss: 0.145071 [57600/60000]\n",
      "Test error : \n",
      " Accuracy : 95.0%, Avg loss : 0.136418 \n",
      "\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n ---\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(train_dataloader, model, loss_fn)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5bc8aa60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved PyTorch Model State to fashion_mnist_base_model.pth\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "class_names = training_data.classes\n",
    "\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'class_names': class_names\n",
    "}, 'fashion_mnist_base_model.pth')\n",
    "print(\"Saved PyTorch Model State to fashion_mnist_base_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e66e4ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction index: 0\n",
      "Predicted class: T-shirt/top\n"
     ]
    }
   ],
   "source": [
    "# Test with custom image!\n",
    "model = CNN().to(device)\n",
    "# Load saved model\n",
    "loaded_model = torch.load(\"fashion_mnist_base_model.pth\")\n",
    "model.load_state_dict(loaded_model['model_state_dict'])\n",
    "model.eval()\n",
    "class_names = loaded_model['class_names']\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "image_path = \"test.jpg\"\n",
    "# Open image and convert to grayscale\n",
    "image = Image.open(image_path).convert(\"L\")\n",
    "\n",
    "# Resize the image to 28x28\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((28,28)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "tensor_image = transform(image)\n",
    "input_tensor = tensor_image.unsqueeze(0).to(device)  # [1, 1, 28, 28]\n",
    "\n",
    "plt.imshow(tensor_image.squeeze(), cmap=\"gray\")\n",
    "plt.title(\"Input Image\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "with torch.inference_mode():\n",
    "    output = model(input_tensor)\n",
    "    # argmax selects the output with highest value\n",
    "    prediction = torch.argmax(output).item()\n",
    "\n",
    "class_name = training_data.classes[prediction]\n",
    "print(f\"Prediction index: {prediction}\")\n",
    "print(f\"Predicted class: {class_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52868ae9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "houdini",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
